{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle competitions list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kaggle datasets download -d andradaolteanu/gtzan-dataset-music-genre-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip gtzan-dataset-music-genre-classification.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio('/content/Data/genres_original/hiphop/hiphop.00001.wav',rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#오디오 파일도 숫자로 변형해서 분석\n",
    "#y 소리가가 떨리는 세기를 시간순서대로 나열열\n",
    "#sr (sampliing rate 1초당 샘플의 개수 )\n",
    "import librosa\n",
    "y,sr = librosa.load('Data/genres_original/reggae/reggae.00036.wav')\n",
    "\n",
    "print(y) #소리가 떨리는 진폭의 배열\n",
    "print(len(y))\n",
    "print(sr) #초당 샘플의 개수 많을수록 음질깨끗??\n",
    "len(y)/sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(y, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "librosa.display.waveshow(y=y,sr=sr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#시간 데이터를 주파수영역 데이터로 변경 분석이 더 잘됨\n",
    "#STFT(Short Time Fourier Transform)\n",
    "#푸리에 변환 : 입력 신호를 다양한 주파수를 가지는 주기함수들로 분해하는 것\n",
    "#1. n_fft : length of the windowed signal after padding with zeros.\n",
    "#            한 번 fft를 해 줄 만큼의 sequence 길이\n",
    "#2. hop_length : window 간의 거리\n",
    "#3. win_length : window 길이..\n",
    "import numpy as np\n",
    "\n",
    "# win_length 는 음성을 작은 조각으로 자를때 작은 조각의 크기입니다.\n",
    "# hop_length 는 음성을 작은 조각으로 자를때 자르는 간격을 의미합니다.\n",
    "# n_mels 는 적용할 mel filter의 개수입니다.\n",
    "\n",
    "D = np.abs(librosa.stft(y,n_fft=2048,hop_length=512))\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(D)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spectogram\n",
    "#시간에 따른 신호 주파수의 스펙트럼 그래프\n",
    "\n",
    "DB = librosa.amplitude_to_db(D,ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "librosa.display.specshow(DB,sr=sr,hop_length=512, x_axis='time' , y_axis='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#알아먹기기 힘든 스펙토그램을 y축을을 mel scale로로 변환하여 좀더 알아먹기편하게 변형\n",
    "#주파수의 단위를 다음 공식에 따라 멜 단위(Mel unit)로 바꾼 스펙트럼을 말한다.\n",
    "#- Mel-scale\n",
    "#Mel-scale은 이러한 pitch에서 발견한 사람의 음을 인지하는 기준(threshold)을 반영한 scale 변환 함수이다.\n",
    "\n",
    "S = librosa.feature.melspectrogram(y=y,sr=sr)\n",
    "S_DB = librosa.amplitude_to_db(S,ref=np.max)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "librosa.display.specshow(S_DB,sr=sr,hop_length=512, x_axis='time' , y_axis='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = librosa.feature.melspectrogram(\n",
    "    y=y,\n",
    "    sr=sr,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    win_length=None,\n",
    "    window='hann',\n",
    "    center=True,\n",
    "    pad_mode='reflect',\n",
    "    power=2.0,\n",
    "    n_mels=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## music_mel로 음향 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('music_mel.npy',spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_new = np.load('music_mel.npy')\n",
    "spec_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = librosa.feature.inverse.mel_to_audio(\n",
    "    spec_new,\n",
    "    sr=sr,\n",
    "    n_fft=2048,\n",
    "    hop_length=512,\n",
    "    win_length=None,\n",
    "    window='hann',\n",
    "    center=True,\n",
    "    pad_mode='reflect',\n",
    "    power=2.0,\n",
    "    n_iter=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile\n",
    "soundfile.write('mel_test.wav',res,22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio('mel_test.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오디오 특성 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#음악의 BPM 추출\n",
    "#BPM은 Beat Per Minute의 약자로써 분당 박자 수\n",
    "tempo,_ = librosa.beat.beat_track(y=y,sr=sr)\n",
    "tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#음파가가 양에서서 음으로 음에서서 양으로로 변환하는값\n",
    "zero_crossings = librosa.zero_crossings(y,pad=False)\n",
    "\n",
    "len(zero_crossings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(y[3000:3500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Harmonics : 사람의 귀로 구분할 수 없는 특징들\n",
    "#Percussives : 리듬과 감정을 나타내는 충격파\n",
    "\n",
    "y_harm, y_perc = librosa.effects.hpss(y)\n",
    "print(y_harm)\n",
    "print(y_perc)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(y_harm,color='b')\n",
    "plt.plot(y_perc,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spectral Centroid\n",
    "#소리를 주파수 표현했을때 , 주파수의 가중평균을 계산하여 소리의 무게중심을 알려줌\n",
    "#블루스음악 무게중심 가운데 , 메탈 마지막\n",
    "import sklearn\n",
    "\n",
    "spectral_centroids = librosa.feature.spectral_centroid(y=y,sr=sr)[0]\n",
    "\n",
    "frames = range(len(spectral_centroids))\n",
    "print(frames)\n",
    "t = librosa.frames_to_time(frames)\n",
    "\n",
    "spec_norm = sklearn.preprocessing.minmax_scale(spectral_centroids,axis=0)\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "librosa.display.waveshow(y=y,sr=sr,alpha=0.5,color='b')\n",
    "plt.plot(t,spec_norm,color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spectral rolloff\n",
    "#신호의 모양을 측정 , 총 스펙트럴 에너지중 낮은 주파수에 얼마나 많이 집중되는지\n",
    "\n",
    "spectral_rolloff = librosa.feature.spectral_rolloff(y=y,sr=sr)[0]\n",
    "print(spectral_rolloff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "def normalize(x,axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n",
    "#MFCCs 오디오 특징들의 집합\n",
    "#음악의 장르를 정하는 것은 사람의 청각\n",
    "#사람의 청각 구조를 반영하여 음성 정보 추출\n",
    "#MFCC란?\n",
    "#MFCC는 오디오 신호에서 추출할 수 있는 feature로, 소리의 고유한 특징을 나타내는 수치입니다.\n",
    "#주로 음성 인식, 화자 인식, 음성 합성, 음악 장르 분류 등 오디오 도메인의 문제를 해결하는 데 사용됩니다.\n",
    "\n",
    "mfccs = librosa.feature.mfcc(y=y,sr=sr)\n",
    "mfccs = normalize(mfccs,axis=1)\n",
    "\n",
    "print(mfccs.mean()) #평균\n",
    "print(mfccs.var())  #분산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chroma frequencies\n",
    "#크로마특징은 음악의 흥미롭고 강렬한 표현\n",
    "#크로마는 인간 청각이 옥타브 차이가 나는 주파수를 가진 두음을 유사음으로 인지\n",
    "\n",
    "chromagram = librosa.feature.chroma_stft(y=y,sr=sr, hop_length=512)\n",
    "chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위의 특징들을 컬럼에 넣어서 유사도 측정\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./Data/features_3_sec.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['filename','length','label'])\n",
    "y = df['label']\n",
    "\n",
    "scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "np_scaled = scaler.fit_transform(X)\n",
    "\n",
    "X = pd.DataFrame(np_scaled, columns=X.columns)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "y = le.transform(y)\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=2222)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Regression, Classification 문제를 모두 지원하며, 성능과 자원 효율이 좋아서, 인기 있게 사용되는 알고리즘\n",
    "#n_estimators 예측까지\n",
    "xgb = XGBClassifier(n_estimators=1000, learning_rate=0.05)\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "y_preds = xgb.predict(X_test)\n",
    "\n",
    "print('Accuracy : ',accuracy_score(y_test,y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "\n",
    "plt.figure()\n",
    "sns.heatmap(cm,annot=True ,\n",
    "            xticklabels=['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock'],\n",
    "            yticklabels=['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock'],\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30 = pd.read_csv('./Data/features_30_sec.csv',index_col='filename')\n",
    "df_30.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_30.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30 = df_30.drop(columns=['length','label'])\n",
    "df_30_scaled = sklearn.preprocessing.scale(df_30)\n",
    "df_30 = pd.DataFrame(df_30_scaled,columns = df_30.columns)\n",
    "df_30.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.core.errors import ConstantInferenceError\n",
    "#코사인유사도\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity(df_30)\n",
    "sim_df = pd.DataFrame(similarity,index=labels.index, columns=labels.index)\n",
    "sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music = sim_df['blues.00000.wav'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music.index[1]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
